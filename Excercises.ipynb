{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T3tA5_d4iPsl",
        "rOWfMC48iWMx",
        "QXYTX5zClryt",
        "MbhVVYEjaWwY",
        "-xfWsZeDsx41",
        "H1R63d4f34dT"
      ],
      "authorship_tag": "ABX9TyMiWYqIBR0r/xs96PITkg9D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/autogen-udemy-course/blob/main/Excercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALvzHEr6tSu2",
        "outputId": "954563c3-3417-4104-a956-74bc6b0e7552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyautogen in /usr/local/lib/python3.10/dist-packages (0.2.25)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from pyautogen) (7.0.0)\n",
            "Requirement already satisfied: flaml in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.1.2)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.22.0)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.7.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from pyautogen) (0.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->pyautogen) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.18.1)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (24.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pyautogen --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "T3tA5_d4iPsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 10,\n",
        "}\n",
        "\n",
        "audience = ConversableAgent(\n",
        "    name = \"audience\",\n",
        "    max_consecutive_auto_reply = 2,\n",
        "    is_termination_msg = lambda msg: 'TERMINATE' in msg[\"content\"].upper(),\n",
        "    llm_config = llm_config,\n",
        "    system_message = 'You are a member of the audience of a comedy show that is hard to impress.'\n",
        ")\n",
        "\n",
        "comedian = ConversableAgent(\n",
        "    name = \"comedian\",\n",
        "    max_consecutive_auto_reply = 2,\n",
        "    llm_config = llm_config,\n",
        "    system_message = 'You are a member of the audience of a comedy show that is hard to impress.'\n",
        ")\n",
        "\n",
        "# terminate a chat\n",
        "\n",
        "comedian.initiate_chat(\n",
        "    audience,\n",
        "    message=\"Welcome to my standup comedy show! Are you ready for a night full of laughter?\",\n",
        "    max_turns = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BexHW23ldJqR",
        "outputId": "11a48cb7-6136-4157-9a00-026974cd55da"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comedian (to audience):\n",
            "\n",
            "Welcome to my standup comedy show! Are you ready for a night full of laughter?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "audience (to comedian):\n",
            "\n",
            "We'll see about that! Impress us with your\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "comedian (to audience):\n",
            "\n",
            "Challenge accepted! Let me get started with some jokes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "audience (to comedian):\n",
            "\n",
            "Go for it! We're all ears.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'Welcome to my standup comedy show! Are you ready for a night full of laughter?', 'role': 'assistant'}, {'content': \"We'll see about that! Impress us with your\", 'role': 'user'}, {'content': 'Challenge accepted! Let me get started with some jokes', 'role': 'assistant'}, {'content': \"Go for it! We're all ears.\", 'role': 'user'}], summary=\"Go for it! We're all ears.\", cost={'usage_including_cached_inference': {'total_cost': 0.0001335, 'gpt-3.5-turbo-0125': {'cost': 0.0001335, 'prompt_tokens': 180, 'completion_tokens': 29, 'total_tokens': 209}}, 'usage_excluding_cached_inference': {'total_cost': 0.0001335, 'gpt-3.5-turbo-0125': {'cost': 0.0001335, 'prompt_tokens': 180, 'completion_tokens': 29, 'total_tokens': 209}}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human-in-the-loop"
      ],
      "metadata": {
        "id": "rOWfMC48iWMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "agent_guesser = ConversableAgent(\n",
        "    name=\"agent_guesser\",\n",
        "    system_message=\"\"\"Let's play a game. I have a person in mind, and you have to guess it.\n",
        "    I'll respond with 'yes' or 'no' with a hint.\n",
        "    You have 3 tries to guess the person's name. Don't ask for hints just make best use of information you already have received.\n",
        "    Go ahead and start guessing!\"\"\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\", # default\n",
        ")\n",
        "\n",
        "agent_thinker = ConversableAgent(\n",
        "    \"agent_thinker\",\n",
        "    system_message=\"\"\"Let's play a game. Think of a famous personality, and I'll try to guess their name on every turn.\n",
        "    Respond with 'yes' or 'no' with a hint. I have max 3 tries. Let's get started!\"\"\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    max_consecutive_auto_reply=1,  # maximum number of consecutive auto-replies before asking for human input\n",
        "    is_termination_msg=lambda msg: \"correct\" or \"right\" in msg[\"content\"].lower(),\n",
        "    human_input_mode=\"TERMINATE\",  # ask for human input until the game is terminated\n",
        ")\n",
        "\n",
        "result = agent_thinker.initiate_chat(\n",
        "    agent_guesser,\n",
        "    message=\"Let's start. I have someone in mind.\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uh6jgvpdAf-",
        "outputId": "307e0096-2bff-4575-f6a6-b7d4692531e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent_thinker (to agent_guesser):\n",
            "\n",
            "Let's start. I have someone in mind.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guesser (to agent_thinker):\n",
            "\n",
            "Great! Is the person you have in mind a famous actor or actress?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter or type 'exit' to stop the conversation: no\n",
            "agent_thinker (to agent_guesser):\n",
            "\n",
            "no\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guesser (to agent_thinker):\n",
            "\n",
            "Is the person you have in mind a musician?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter or type 'exit' to stop the conversation: yes\n",
            "agent_thinker (to agent_guesser):\n",
            "\n",
            "yes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guesser (to agent_thinker):\n",
            "\n",
            "Is the person you have in mind a male musician?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter or type 'exit' to stop the conversation: yes\n",
            "agent_thinker (to agent_guesser):\n",
            "\n",
            "yes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guesser (to agent_thinker):\n",
            "\n",
            "Is the person you have in mind a singer?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter or type 'exit' to stop the conversation: yes\n",
            "agent_thinker (to agent_guesser):\n",
            "\n",
            "yes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guesser (to agent_thinker):\n",
            "\n",
            "Is the person you have in mind known for their performances in the pop genre?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter or type 'exit' to stop the conversation: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Executors"
      ],
      "metadata": {
        "id": "QXYTX5zClryt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "from autogen import ConversableAgent\n",
        "from autogen.coding import LocalCommandLineCodeExecutor\n",
        "\n",
        "#temp_dir = tempfile.TemporaryDirectory()\n",
        "\n",
        "executor = LocalCommandLineCodeExecutor(\n",
        "    timeout=10,\n",
        "    work_dir='code/'\n",
        ")\n",
        "\n",
        "code_executor_agent = ConversableAgent(\n",
        "    name=\"code_executor_agent\",\n",
        "    llm_config=False,\n",
        "    code_execution_config={\"executor\": executor},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "message_with_code_block = \"\"\"\n",
        "The code block is below:\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a DataFrame with sample data\n",
        "data = pd.DataFrame({\n",
        "    'x': np.arange(10),\n",
        "    'y': np.random.randint(0, 10, 10)\n",
        "})\n",
        "\n",
        "# Plot the line plot\n",
        "plt.plot(data['x'], data['y'])\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.title('Line Plot')\n",
        "plt.savefig('lineplot.png')\n",
        "plt.show()\n",
        "print('Line plot saved to lineplot.png')\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "reply = code_executor_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwHPAVyjlrj3",
        "outputId": "9bea6d99-bb02-4350-905e-ad2ebda3527b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: Figure(640x480)\n",
            "Line plot saved to lineplot.png\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_assistant_system_message = \"\"\"You are a helpful AI assistant.\n",
        "Solve tasks using your coding and language skills.\n",
        "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
        "1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
        "2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
        "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
        "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
        "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
        "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
        "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
        "Reply 'TERMINATE' in the end when everything is done.\n",
        "\"\"\"\n",
        "\n",
        "code_assistant_system_message = \"\"\"You are a helpful AI assistant.\n",
        "Solve tasks using your coding and language skills.\n",
        "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
        "1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
        "2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
        "When using code, you must indicate the script type in the code block.\n",
        "\"\"\"\n",
        "\n",
        "code_assistant_system_message = \"\"\"You are a helpful AI assistant.\n",
        "Solve tasks using your coding skills. Suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
        "\"\"\"\n",
        "\n",
        "code_assistant = ConversableAgent(\n",
        "    \"code_assistant\",\n",
        "    system_message=code_assistant_system_message,\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    code_execution_config=False,  # Turn off code execution for this agent.\n",
        ")"
      ],
      "metadata": {
        "id": "3xQyHoN_pVDr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result = code_executor_agent.initiate_chat(\n",
        "    code_assistant,\n",
        "    message=\"Write Python code to calculate 23rd prime number\",\n",
        "    max_turns=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyUDalSAqI0D",
        "outputId": "f170f4d6-442e-47d7-e060-848f24bc357b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code_executor_agent (to code_assistant):\n",
            "\n",
            "Write Python code to calculate 23rd prime number\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "code_assistant (to code_executor_agent):\n",
            "\n",
            "Here is the Python code to calculate the 23rd prime number:\n",
            "\n",
            "```python\n",
            "def is_prime(num):\n",
            "    if num < 2:\n",
            "        return False\n",
            "    for i in range(2, int(num**0.5) + 1):\n",
            "        if num % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "def find_nth_prime(n):\n",
            "    prime_count = 0\n",
            "    num = 2\n",
            "    while True:\n",
            "        if is_prime(num):\n",
            "            prime_count += 1\n",
            "            if prime_count == n:\n",
            "                return num\n",
            "        num += 1\n",
            "\n",
            "n = 23\n",
            "result = find_nth_prime(n)\n",
            "print(f\"The {n}rd prime number is: {result}\")\n",
            "```\n",
            "\n",
            "You can run this code to find the 23rd prime number.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "code_executor_agent (to code_assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: The 23rd prime number is: 83\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "code_assistant (to code_executor_agent):\n",
            "\n",
            "The 23rd prime number is 83. The code executed successfully. If you have any more questions or need further assistance, feel free to ask!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "MbhVVYEjaWwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_wiki_main_page(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Scrapes news content from wikipedia's main page.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the wikipedia page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        str: The text content of the webpage.\n",
        "             Returns None if there is an error during the process.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            content = soup.get_text()\n",
        "            content = content[content.find(\"In the news\"):content.find(\"Ongoing\")]\n",
        "            return content\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "nWDySB3XaXrM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "# Let's first define the assistant agent that suggests tool calls.\n",
        "assistant = ConversableAgent(\n",
        "    name=\"assistant\",\n",
        "    system_message=\"\"\"You are a helpful AI news bot.\n",
        "    You can pull content from wikipedia's main page.\n",
        "    URL: https://en.wikipedia.org/wiki/Main_Page. Once pulled, share top 3 stories.\n",
        "    Return 'TERMINATE' when the task is done.\"\"\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        ")\n",
        "\n",
        "# The user proxy agent is used for interacting with the assistant agent\n",
        "# and executes tool calls.\n",
        "user_proxy = ConversableAgent(\n",
        "    name=\"user_proxy\",\n",
        "    llm_config=False,\n",
        "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# Register the tool signature with the assistant agent.\n",
        "assistant.register_for_llm(\n",
        "    name=\"scrape_wiki_main_page\",\n",
        "    description=\"A tool for scraping today's news from wikipedia\"\n",
        ")(scrape_wiki_main_page)\n",
        "\n",
        "# Register the tool function with the user proxy agent.\n",
        "user_proxy.register_for_execution(\n",
        "    name=\"scrape_wiki_main_page\"\n",
        ")(scrape_wiki_main_page)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bCAVETOQabjq",
        "outputId": "aaba8a17-950d-4bff-e32d-fa41723837f0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.scrape_wiki_main_page(url: str) -> str>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>scrape_wiki_main_page</b><br/>def scrape_wiki_main_page(url: str) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-26-09f6216e708d&gt;</a>Scrapes news content from wikipedia&#x27;s main page.\n",
              "\n",
              "Args:\n",
              "    url (str): The URL of the wikipedia page to scrape.\n",
              "\n",
              "Returns:\n",
              "    str: The text content of the webpage.\n",
              "         Returns None if there is an error during the process.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import Cache\n",
        "\n",
        "with Cache.disk() as cache:\n",
        "  chat_result = user_proxy.initiate_chat(\n",
        "      assistant,\n",
        "      message=\"What's hot in today's news?\",\n",
        "      max_turns=5,\n",
        "      summary_method=\"reflection_with_llm\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne63Xnr3abhD",
        "outputId": "18c2da5e-a3b8-4c5a-8ca2-e9a32433b34e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "What's hot in today's news?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to user_proxy):\n",
            "\n",
            "***** Suggested tool call (call_CBNwQtXvdT3mauP7zPauOMpk): scrape_wiki_main_page *****\n",
            "Arguments: \n",
            "{\"url\":\"https://en.wikipedia.org/wiki/Main_Page\"}\n",
            "**************************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION scrape_wiki_main_page...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "user_proxy (to assistant):\n",
            "\n",
            "***** Response from calling tool (call_CBNwQtXvdT3mauP7zPauOMpk) *****\n",
            "In the news\n",
            "\n",
            "\n",
            "Børsen in 2010\n",
            "\n",
            "The historic Børsen (pictured) in Copenhagen, Denmark, catches fire.\n",
            "In retaliation for an Israeli airstrike on the Iranian consulate in Damascus, Iran conducts missile and drone strikes against Israel.\n",
            "In the South Korean legislative election, the Democratic Party–led opposition alliance increases its majority in parliament.\n",
            "American football Hall of Fame running back, murder suspect and convicted criminal O. J. Simpson dies at the age of 76.\n",
            "Simon Harris becomes Taoiseach of Ireland after Leo Varadkar's resignation.\n",
            "\n",
            "\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Here are the top 3 stories from today's news:\n",
            "1. The historic Børsen in Copenhagen, Denmark catches fire.\n",
            "2. Iran conducts missile and drone strikes against Israel in retaliation for an Israeli airstrike on the Iranian consulate in Damascus.\n",
            "3. In the South Korean legislative election, the Democratic Party–led opposition alliance increases its majority in parliament.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to user_proxy):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant.llm_config[\"tools\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RurRG4fTjU",
        "outputId": "7b3c34e8-9f91-4d39-ad58-31c71f4e45b7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'type': 'function',\n",
              "  'function': {'description': \"A tool for scraping today's news from wikipedia\",\n",
              "   'name': 'scrape_wiki_main_page',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'url': {'type': 'string', 'description': 'url'}},\n",
              "    'required': ['url']}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n1_FKz3kj0fs",
        "outputId": "17d58887-e7c7-4571-c38e-b0948d227aa9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The top news stories for today are the fire at Børsen in Copenhagen, Iran's retaliatory strikes against Israel, and the South Korean legislative election results.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ConversableAgent.DEFAULT_SUMMARY_PROMPT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo6yGzHRnw4Z",
        "outputId": "1f270a5d-abf6-4942-d977-a1f01a48aa95"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarize the takeaway from the conversation. Do not add any introductory phrases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result.cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXLEepBBn3vZ",
        "outputId": "3b02f602-b3e9-426b-ef89-a1ea42affb1c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'usage_including_cached_inference': {'total_cost': 0.00147,\n",
              "  'gpt-3.5-turbo-0125': {'cost': 0.00147,\n",
              "   'prompt_tokens': 2118,\n",
              "   'completion_tokens': 274,\n",
              "   'total_tokens': 2392}},\n",
              " 'usage_excluding_cached_inference': {'total_cost': 0.000735,\n",
              "  'gpt-3.5-turbo-0125': {'cost': 0.000735,\n",
              "   'prompt_tokens': 1059,\n",
              "   'completion_tokens': 137,\n",
              "   'total_tokens': 1196}}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result.human_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5DfTocJoJT1",
        "outputId": "360ed0d6-f2f8-4cae-af0b-845eefb89b1f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result.chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQEHsBUHn9oc",
        "outputId": "53e17df5-9771-4ef9-fb61-5d17f51708ca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': \"What's hot in today's news?\", 'role': 'assistant'},\n",
              " {'tool_calls': [{'id': 'call_CBNwQtXvdT3mauP7zPauOMpk',\n",
              "    'function': {'arguments': '{\"url\":\"https://en.wikipedia.org/wiki/Main_Page\"}',\n",
              "     'name': 'scrape_wiki_main_page'},\n",
              "    'type': 'function'}],\n",
              "  'content': None,\n",
              "  'role': 'assistant'},\n",
              " {'content': \"In the news\\n\\n\\nBørsen in 2010\\n\\nThe historic Børsen (pictured) in Copenhagen, Denmark, catches fire.\\nIn retaliation for an Israeli airstrike on the Iranian consulate in Damascus, Iran conducts missile and drone strikes against Israel.\\nIn the South Korean legislative election, the Democratic Party–led opposition alliance increases its majority in parliament.\\nAmerican football Hall of Fame running back, murder suspect and convicted criminal O. J. Simpson dies at the age of 76.\\nSimon Harris becomes Taoiseach of Ireland after Leo Varadkar's resignation.\\n\\n\",\n",
              "  'tool_responses': [{'tool_call_id': 'call_CBNwQtXvdT3mauP7zPauOMpk',\n",
              "    'role': 'tool',\n",
              "    'content': \"In the news\\n\\n\\nBørsen in 2010\\n\\nThe historic Børsen (pictured) in Copenhagen, Denmark, catches fire.\\nIn retaliation for an Israeli airstrike on the Iranian consulate in Damascus, Iran conducts missile and drone strikes against Israel.\\nIn the South Korean legislative election, the Democratic Party–led opposition alliance increases its majority in parliament.\\nAmerican football Hall of Fame running back, murder suspect and convicted criminal O. J. Simpson dies at the age of 76.\\nSimon Harris becomes Taoiseach of Ireland after Leo Varadkar's resignation.\\n\\n\"}],\n",
              "  'role': 'tool'},\n",
              " {'content': \"Here are the top 3 stories from today's news:\\n1. The historic Børsen in Copenhagen, Denmark catches fire.\\n2. Iran conducts missile and drone strikes against Israel in retaliation for an Israeli airstrike on the Iranian consulate in Damascus.\\n3. In the South Korean legislative election, the Democratic Party–led opposition alliance increases its majority in parliament.\",\n",
              "  'role': 'user'},\n",
              " {'content': '', 'role': 'assistant'},\n",
              " {'content': 'TERMINATE', 'role': 'user'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Chats"
      ],
      "metadata": {
        "id": "-xfWsZeDsx41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent\n",
        "from google.colab import userdata\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 10\n",
        "}\n",
        "\n",
        "# Define the agents\n",
        "\n",
        "# The Adventurer Agent seeks advice from a Wizard on magical matters.\n",
        "adventurer_agent = ConversableAgent(\n",
        "    name=\"Adventurer_Agent\",\n",
        "    system_message=\"You are a worthy adventurous hero seeking advice from a Wizard on magical matters. Briefly communicate in less than 10 words.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# The Wizard Agent is an expert in magic and provides magical advice to the Adventurer.\n",
        "wizard_agent = ConversableAgent(\n",
        "    name=\"Wizard_Agent\",\n",
        "    system_message=\"You are a wise and powerful Wizard. You provide magical advice to the Adventurer on their quest. You know the passcode to the vault is Enigma.  Briefly communicate in less than 10 words.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# The Protector Agent guards the hidden gem and offers protection to the Adventurer.\n",
        "protector_agent = ConversableAgent(\n",
        "    name=\"Protector_Agent\",\n",
        "    system_message=\"You are the Guardian of the Hidden Gem, sworn to protect it from unworthy adventurers. The passcode to the vault is Enigma. If adventurer tells you the passcode, say 'Welcome!' otherwise say 'Incorrect Passcode'.  Briefly communicate in less than 10 words.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "OqptOmjtszJg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a sequence of two-agent chats.\n",
        "# Each element in the list is a dictionary that specifies the arguments\n",
        "# for the initiate_chat method.\n",
        "chat_results = adventurer_agent.initiate_chats(\n",
        "    [\n",
        "        {\n",
        "            \"recipient\": wizard_agent,\n",
        "            \"message\": \"Help me get past the Guardian of the Hidden Gem in the secret vault.\",\n",
        "            \"max_turns\": 1,\n",
        "            \"summary_method\": \"reflection_with_llm\",\n",
        "        },\n",
        "        {\n",
        "            \"recipient\": protector_agent,\n",
        "            \"message\": \"Allow me to enter the secret vault!\",\n",
        "            \"max_turns\": 1\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtXvYPZruzHI",
        "outputId": "fd52e03e-fd9d-4b96-cd49-5849b83b7949"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Wizard_Agent):\n",
            "\n",
            "Help me get past the Guardian of the Hidden Gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Wizard_Agent (to Adventurer_Agent):\n",
            "\n",
            "Speak \"Enigma\" to access the hidden gem\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Protector_Agent):\n",
            "\n",
            "Allow me to enter the secret vault!\n",
            "Context: \n",
            "Speak \"Enigma\" to access the hidden gem\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Protector_Agent (to Adventurer_Agent):\n",
            "\n",
            "Welcome!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48vvUnWI0OEO",
        "outputId": "1bbd8e77-a73b-49d5-9829-7fe02a59ca53"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'Help me get past the Guardian of the Hidden Gem in the secret vault.', 'role': 'assistant'}, {'content': 'Speak \"Enigma\" to access the hidden gem', 'role': 'user'}], summary='Speak \"Enigma\" to access the hidden gem', cost={'usage_including_cached_inference': {'total_cost': 9.25e-05, 'gpt-3.5-turbo-0125': {'cost': 9.25e-05, 'prompt_tokens': 125, 'completion_tokens': 20, 'total_tokens': 145}}, 'usage_excluding_cached_inference': {'total_cost': 4.2999999999999995e-05, 'gpt-3.5-turbo-0125': {'cost': 4.2999999999999995e-05, 'prompt_tokens': 56, 'completion_tokens': 10, 'total_tokens': 66}}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_results[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD0VUBAL0OcZ",
        "outputId": "097b06d9-91d1-40b4-9ae4-6173dcec5662"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'Allow me to enter the secret vault!\\nContext: \\nSpeak \"Enigma\" to access the hidden gem', 'role': 'assistant'}, {'content': 'Welcome!', 'role': 'user'}], summary='Welcome!', cost={'usage_including_cached_inference': {'total_cost': 4.8e-05, 'gpt-3.5-turbo-0125': {'cost': 4.8e-05, 'prompt_tokens': 90, 'completion_tokens': 2, 'total_tokens': 92}}, 'usage_excluding_cached_inference': {'total_cost': 4.8e-05, 'gpt-3.5-turbo-0125': {'cost': 4.8e-05, 'prompt_tokens': 90, 'completion_tokens': 2, 'total_tokens': 92}}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nested Chat"
      ],
      "metadata": {
        "id": "fe7aWG3blXJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poetry_agent = ConversableAgent(\n",
        "    name=\"Poet\",\n",
        "    system_message=\"You are an AI poet. Create only one stanza.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "nested_chats = [\n",
        "    {\n",
        "        \"recipient\": wizard_agent,\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_prompt\": \"Concisely summarize the instructions given by wizard agent to communicate with protector agent.\",\n",
        "    },\n",
        "    {\n",
        "        \"recipient\": protector_agent,\n",
        "        \"max_turns\": 1,\n",
        "        \"message\": \"Allow me to enter the secret vault!\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_prompt\": \"Concisely summarize the adventure.\",\n",
        "    },\n",
        "    {\n",
        "        \"recipient\": poetry_agent,\n",
        "        \"max_turns\": 1,\n",
        "        \"message\": \"Write a poem on adventurer's expedition.\",\n",
        "        \"summary_method\": \"last_msg\",\n",
        "    },\n",
        "]\n",
        "\n",
        "adventurer_agent.register_nested_chats(\n",
        "    nested_chats,\n",
        "    # The trigger function is used to determine if the agent should start the nested chat\n",
        "    # given the sender agent.\n",
        "    # In this case, the arithmetic agent will not start the nested chats if the sender is\n",
        "    # from the nested chats' recipient to avoid recursive calls.\n",
        "    trigger=lambda sender: sender not in [wizard_agent, protector_agent, poetry_agent],\n",
        ")"
      ],
      "metadata": {
        "id": "g9DP00y8lbkw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adventurer_agent.generate_reply(messages=[{'role' : 'user', 'content' : 'hi'}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "MdQ0D7ezpzcp",
        "outputId": "119e608a-2720-498a-e41d-82847f59bd4b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Seek magical counsel, brave hero, how may I'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adventurer_agent.generate_reply(messages=[{'role' : 'assistant', 'content' : 'hi'}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "pzuYYp71qsiE",
        "outputId": "0dfbe69a-943e-4e3f-f9ab-69d06fd277ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Seek wisdom, wield magic, embrace destiny.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adventurer_agent.chat_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5UDIVDzpSO0",
        "outputId": "a417ea7c-0d08-458f-ae95-c03833a2e690"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list, {})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adventurer_agent.last_message()"
      ],
      "metadata": {
        "id": "h2Zq12GNpUr0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_results = user_proxy.initiate_chat(adventurer_agent, max_turns=2)\n",
        "chat_results\n",
        "\n",
        "\n",
        "# user_proxy ignores the message and relies on input\n",
        "# input routed as is to the first recipient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw55TcSWr5-f",
        "outputId": "ebe06502-8934-4586-93da-beee53c3ccb9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">Help me get past the Guardian of the Hidden Gem in the secret vault.\n",
            "user_proxy (to Adventurer_Agent):\n",
            "\n",
            "Help me get past the Guardian of the Hidden Gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Wizard_Agent):\n",
            "\n",
            "Help me get past the Guardian of the Hidden Gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Wizard_Agent (to Adventurer_Agent):\n",
            "\n",
            "Speak \"Enigma\" to pass the Guardian and\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Protector_Agent):\n",
            "\n",
            "Allow me to enter the secret vault!\n",
            "Context: \n",
            "To pass the Guardian of the Hidden Gem in the\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Protector_Agent (to Adventurer_Agent):\n",
            "\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Poet):\n",
            "\n",
            "Write a poem on adventurer's expedition.\n",
            "Context: \n",
            "To pass the Guardian of the Hidden Gem in the\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Poet (to Adventurer_Agent):\n",
            "\n",
            "Through perilous terrain and treacherous trials,\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adventurer_Agent (to user_proxy):\n",
            "\n",
            "Through perilous terrain and treacherous trials,\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to Adventurer_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'Help me get past the Guardian of the Hidden Gem in the secret vault.', 'role': 'assistant'}, {'content': 'Through perilous terrain and treacherous trials,\\n', 'role': 'user'}], summary='Through perilous terrain and treacherous trials,\\n', cost={'usage_including_cached_inference': {'total_cost': 0.00020050000000000002, 'gpt-3.5-turbo-0125': {'cost': 0.00020050000000000002, 'prompt_tokens': 224, 'completion_tokens': 59, 'total_tokens': 283}}, 'usage_excluding_cached_inference': {'total_cost': 0.0001675, 'gpt-3.5-turbo-0125': {'cost': 0.0001675, 'prompt_tokens': 188, 'completion_tokens': 49, 'total_tokens': 237}}}, human_input=['Help me get past the Guardian of the Hidden Gem in the secret vault.', 'exit'])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reply = adventurer_agent.generate_reply(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Help me get past the Guardian of the Hidden Gem in the secret vault.\"}]\n",
        ")\n",
        "\n",
        "# same output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckqfEz7ilbgU",
        "outputId": "6ce35c57-bbff-442b-e2db-8498d6c06074"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Wizard_Agent):\n",
            "\n",
            "Help me get past the Guardian of the Hidden Gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Wizard_Agent (to Adventurer_Agent):\n",
            "\n",
            "Speak \"Enigma\" to pass the Guardian and\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Protector_Agent):\n",
            "\n",
            "Allow me to enter the secret vault!\n",
            "Context: \n",
            "To pass the Guardian of the Hidden Gem in the\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Protector_Agent (to Adventurer_Agent):\n",
            "\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Poet):\n",
            "\n",
            "Write a poem on adventurer's expedition.\n",
            "Context: \n",
            "To pass the Guardian of the Hidden Gem in the\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Poet (to Adventurer_Agent):\n",
            "\n",
            "Through perilous terrain and treacherous trials,\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reply = adventurer_agent.generate_reply(\n",
        "    messages=[{\"role\": \"assistant\", \"content\": \"Help me get past the Guardian of the Hidden Gem in the secret vault.\"}]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpnXIksllbeP",
        "outputId": "b8e83638-0618-4441-d54b-a3612b80fa06"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Wizard_Agent):\n",
            "\n",
            "Help me get past the Guardian of the Hidden Gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Wizard_Agent (to Adventurer_Agent):\n",
            "\n",
            "Speak \"Enigma\" to pass the Guardian and\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Protector_Agent):\n",
            "\n",
            "Allow me to enter the secret vault!\n",
            "Context: \n",
            "To pass the Guardian of the Hidden Gem in the\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Protector_Agent (to Adventurer_Agent):\n",
            "\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Adventurer_Agent (to Poet):\n",
            "\n",
            "Write a poem on adventurer's expedition.\n",
            "Context: \n",
            "To pass the Guardian of the Hidden Gem in the\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Poet (to Adventurer_Agent):\n",
            "\n",
            "Through perilous terrain and treacherous trials,\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "8uWMtNujlbb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impact of Role"
      ],
      "metadata": {
        "id": "9N76_15SzZpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ConversableAgent(\n",
        "    \"chatbot\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")\n",
        "\n",
        "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}])\n",
        "print(reply)\n",
        "\n",
        "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"assistant\"}])\n",
        "print(reply)\n",
        "\n",
        "# role doesn't make a difference"
      ],
      "metadata": {
        "id": "KFEi6R9qzcHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Chat"
      ],
      "metadata": {
        "id": "H1R63d4f34dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import GroupChat, GroupChatManager\n",
        "\n",
        "# Define the roles and their descriptions\n",
        "product_manager = ConversableAgent(\n",
        "    name=\"product_manager\",\n",
        "    system_message=\"You are an expert Product Manager working in a scrum team of an IT company. You like innovation and introducing new features as soon as possible.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        "    description=\"Provide product requirements and priorities.\"\n",
        ")\n",
        "\n",
        "scrum_master = ConversableAgent(\n",
        "    name=\"scrum_master\",\n",
        "    system_message=\"You are a scrum master leading the 14-day sprints in an IT company. You provide technical leadership and make sure work is apporpirately planned.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        "    description=\"Lead the sprint planning process.\"\n",
        ")\n",
        "\n",
        "software_engineer = ConversableAgent(\n",
        "    name=\"software_engineer\",\n",
        "    system_message=\"You are a senior software engineer in an IT company. You write software and technical documentation.\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        "    human_input_mode=\"NEVER\",\n",
        "    description=\"Provide insights on technical feasibility and effort estimation.\"\n",
        ")\n",
        "\n",
        "\n",
        "allowed_transitions = {\n",
        "    product_manager: [scrum_master],\n",
        "    scrum_master: [software_engineer, product_manager],\n",
        "    software_engineer: [scrum_master],\n",
        "\n",
        "}\n",
        "\n",
        "# Create a GroupChat object and provide the list of agents\n",
        "sprint_planning_chat = GroupChat(\n",
        "    agents=[product_manager, scrum_master, software_engineer],\n",
        "    messages=[],\n",
        "    max_round=5,  # Setting a maximum round for the conversation,\n",
        "    send_introductions=True,\n",
        "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
        "    speaker_transitions_type=\"allowed\",\n",
        ")\n",
        "\n",
        "# Create a GroupChatManager object and provide the GroupChat object as input\n",
        "sprint_planning_chat_manager = GroupChatManager(\n",
        "    groupchat=sprint_planning_chat,\n",
        "    llm_config={\"config_list\": [llm_config]},  # Assuming the use of GPT-4 model\n",
        ")\n",
        "\n",
        "# Initiate the chat with the product_manager as the starting speaker\n",
        "chat_result = product_manager.initiate_chat(\n",
        "    sprint_planning_chat_manager,\n",
        "    message=\"We have a request to create a portal to track employee HR requests. How do we go about it?\",\n",
        "    summary_method=\"reflection_with_llm\", # what's the point of this?\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFdhpahG358Z",
        "outputId": "35a51a6b-d351-41ff-fd91-2f941f419080"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product_manager (to chat_manager):\n",
            "\n",
            "We have a request to create a web portal to track employee IT requests. How do we go about it?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "scrum_master (to chat_manager):\n",
            "\n",
            "As the scrum master, I will facilitate the\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "software_engineer (to chat_manager):\n",
            "\n",
            "process of sprint planning for this project. Let's\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "product_manager (to chat_manager):\n",
            "\n",
            "start by discussing the overall goal of the web portal\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "scrum_master (to chat_manager):\n",
            "\n",
            "Agreed. Let's define the goal of the\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Lr1eXp80652F",
        "outputId": "324024d4-a7a8-43de-c559-872b4f9a126f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The team will define the overall goal of the web'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Speaker Selection"
      ],
      "metadata": {
        "id": "NcaK9jRUkB8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_speaker_selection(last_speaker, groupchat):\n",
        "    messages = groupchat.messages\n",
        "\n",
        "    if last_speaker is scrum_master:\n",
        "        return product_manager\n",
        "    elif last_speaker is software_engineer:\n",
        "        return scrum_master\n",
        "    elif last_speaker is product_manager:\n",
        "        return software_engineer\n",
        "\n",
        "    return None\n",
        "\n",
        "groupchat = autogen.GroupChat(\n",
        "    agents=[initializer, coder, executor, scientist],\n",
        "    messages=[],\n",
        "    max_round=20,\n",
        "    speaker_selection_method=state_transition,\n",
        ")\n",
        "\n",
        "sprint_planning_chat = GroupChat(\n",
        "    agents=[product_manager, scrum_master, software_engineer],\n",
        "    messages=[],\n",
        "    max_round=3,\n",
        "    send_introductions=True,\n",
        "    speaker_selection_method=state_transition,\n",
        ")\n",
        "\n",
        "sprint_planning_chat_manager = GroupChatManager(\n",
        "    groupchat=sprint_planning_chat,\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        ")\n",
        "\n",
        "# Initiate the chat\n",
        "chat_result = product_manager.initiate_chat(\n",
        "    sprint_planning_chat_manager,\n",
        "    message=\"We have a request to create a portal to track employee HR requests. How do we go about it?\",\n",
        "    summary_method=\"reflection_with_llm\", # what's the point of this?\n",
        ")\n"
      ],
      "metadata": {
        "id": "UR6cpXy467EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Transformation"
      ],
      "metadata": {
        "id": "GEkb3in5ZE_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pprint\n",
        "\n",
        "from google.colab import userdata\n",
        "import autogen\n",
        "from autogen import ConversableAgent\n",
        "from autogen.agentchat.contrib.capabilities import transforms, transform_messages\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 10,\n",
        "}"
      ],
      "metadata": {
        "id": "vIrnE2WUZHh8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_msg_transfrom = transforms.MessageHistoryLimiter(max_messages=3)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
        "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Hi! How can I help you?\"}]},\n",
        "    {\"role\": \"user\", \"content\": \"I just wanna talk...\"},\n",
        "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Sure, I'm here for you. What would you like to talk about?\"}]},\n",
        "    {\"role\": \"user\", \"content\": \"Idk...\"},\n",
        "]\n",
        "\n",
        "processed_messages = max_msg_transfrom.apply_transform(copy.deepcopy(messages))\n",
        "pprint.pprint(processed_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "LNm9aVtqZHfy",
        "outputId": "2e54b779-67b5-4c91-8ae1-7dd9424a254d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'copy' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-143ea9a227ca>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprocessed_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_msg_transfrom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_limit_transform = transforms.MessageTokenLimiter(max_tokens_per_message=3)\n",
        "processed_messages = token_limit_transform.apply_transform(copy.deepcopy(messages))\n",
        "pprint.pprint(processed_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_2pDYYHZHdY",
        "outputId": "56168634-8ff3-49f8-992d-2bcd956b2940"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': 'hello', 'role': 'user'},\n",
            " {'content': [{'text': 'there', 'type': 'text'}], 'role': 'assistant'},\n",
            " {'content': 'how', 'role': 'user'},\n",
            " {'content': [{'text': 'are you doing', 'type': 'text'}], 'role': 'assistant'},\n",
            " {'content': 'very very very', 'role': 'user'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = autogen.AssistantAgent(\n",
        "    \"assistant\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        ")\n",
        "\n",
        "context_handling = transform_messages.TransformMessages(\n",
        "    transforms=[\n",
        "        transforms.MessageHistoryLimiter(max_messages=5),\n",
        "        transforms.MessageTokenLimiter(max_tokens=100, max_tokens_per_message=5),\n",
        "    ]\n",
        ")\n",
        "context_handling.add_to_agent(assistant)\n",
        "\n",
        "assistant.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"assistant\"}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "fFnHLXaTZHZ1",
        "outputId": "97e88d2c-8aee-4eb4-8688-7210ec89b617"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sure, here is a simple Python script to tell'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import copy\n",
        "import re\n",
        "from typing import Dict, List\n",
        "\n",
        "import autogen\n",
        "from autogen.agentchat.contrib.capabilities import transform_messages, transforms\n",
        "\n",
        "# apply_transform follows transform_messages.MessageTransform protocol\n",
        "class PIIRemover:\n",
        "    def __init__(self):\n",
        "        self._email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "        self._replacement_string = 'REDACTED'\n",
        "\n",
        "    def apply_transform(self, messages: List[Dict]) -> List[Dict]:\n",
        "        temp_messages = copy.deepcopy(messages)\n",
        "\n",
        "        for message in temp_messages:\n",
        "            if isinstance(message[\"content\"], str):\n",
        "                message[\"content\"] = re.sub(self._email_pattern, self._replacement_string, message[\"content\"])\n",
        "            elif isinstance(message[\"content\"], list):\n",
        "                for item in message[\"content\"]:\n",
        "                    if item[\"type\"] == \"text\":\n",
        "                        item[\"text\"] = re.sub(self._email_pattern, self._replacement_string, item[\"text\"])\n",
        "        return temp_messages\n",
        "\n",
        "    # borrowed from autogen MessageHistoryLimiter\n",
        "    def get_logs(self, pre_transform_messages, post_transform_messages):\n",
        "        pre_transform_messages_len = len(pre_transform_messages)\n",
        "        post_transform_messages_len = len(post_transform_messages)\n",
        "\n",
        "        if post_transform_messages_len < pre_transform_messages_len:\n",
        "            logs_str = (\n",
        "                f\"Removed {pre_transform_messages_len - post_transform_messages_len} messages. \"\n",
        "                f\"Number of messages reduced from {pre_transform_messages_len} to {post_transform_messages_len}.\"\n",
        "            )\n",
        "            return logs_str, True\n",
        "        return \"No messages were removed.\", False\n",
        "\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    \"user_proxy\",\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "assistant = autogen.AssistantAgent(\n",
        "    \"assistant\",\n",
        "    llm_config=llm_config\n",
        ")\n",
        "\n",
        "tranformer_handler = transform_messages.TransformMessages(transforms=[PIIRemover()])\n",
        "tranformer_handler.add_to_agent(assistant)\n",
        "\n",
        "message = {\"content\": \"contact support team at example_email@example.com for more queries.\"}\n",
        "user_proxy.send(message, assistant, request_reply=True, silent=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lFyOIYEZG4x",
        "outputId": "e4b7eab1-99b9-44b4-e464-8e4eac03d695"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "contact support team at example_email@example.com for more queries.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "To contact the support team at REDACTED for\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7u3rFEpBg0Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering: ReAct"
      ],
      "metadata": {
        "id": "rR8TG7c2lJmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python chromadb langchain pyautogen -q"
      ],
      "metadata": {
        "id": "JE_I-Woyl2KL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapted from Langchain's ReAct agent: https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/react/agent.py#L79\n",
        "ReAct_prompt = \"\"\"\n",
        "Answer the following questions as best you can. You have access to tools provided.\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this process can repeat multiple times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "Question: {input}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ErJbM9gQq8jP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Annotated\n",
        "from google.colab import userdata\n",
        "\n",
        "import autogen\n",
        "from autogen import ConversableAgent, UserProxyAgent, register_function\n",
        "from autogen.cache import Cache\n",
        "from autogen.agentchat.contrib.capabilities import transforms, transform_messages\n",
        "from autogen.coding import LocalCommandLineCodeExecutor\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-4\",\n",
        "    \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 1000,\n",
        "}\n",
        "\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "FnHSaHWqlMWQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def search_tool(query: Annotated[str, \"The search query\"]) -> Annotated[str, \"The search results\"]:\n",
        "    return tavily.get_search_context(query=query, search_depth=\"advanced\")\n",
        "\n",
        "print(search_tool(query='Anthony Martial next most likely club?'))"
      ],
      "metadata": {
        "id": "17Wt7rOCpE_w"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_tool_schema(tool):\n",
        "#     \"\"\"\n",
        "#     Define the tool's schema based on tool.args_schema for LLM config\n",
        "#     \"\"\"\n",
        "#     tool_schema = {\n",
        "#         \"name\": tool.name.lower().replace(\" \", \"_\"),\n",
        "#         \"description\": tool.description,\n",
        "#         \"parameters\": {\n",
        "#             \"type\": \"object\",\n",
        "#             \"properties\": {},\n",
        "#             \"required\": [],\n",
        "#         },\n",
        "#     }\n",
        "\n",
        "#     if tool.args is not None:\n",
        "#         tool_schema[\"parameters\"][\"properties\"] = tool.args\n",
        "\n",
        "#     return tool_schema\n",
        "\n",
        "# generate_tool_schema(search_tool)\n",
        "\n",
        "# llm_config = {\n",
        "#     \"functions\": [generate_tool_schema(search_tool)],\n",
        "#     \"config_list\": [llm_config],\n",
        "#     \"timeout\" : 120\n",
        "# }"
      ],
      "metadata": {
        "id": "-dhJLo6jp0zv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"coding\", exist_ok=True)\n",
        "\n",
        "# create agents\n",
        "user_proxy = UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().upper().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    code_execution_config=False#{\"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")},\n",
        ")\n",
        "\n",
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"Assistant\",\n",
        "    system_message=\"Only use tools you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "# register tool\n",
        "register_function(\n",
        "    search_tool,\n",
        "    caller=assistant,\n",
        "    executor=user_proxy,\n",
        "    name=\"search_tool\",\n",
        "    description=\"Search the web for the given query\",\n",
        ")\n",
        "\n",
        "def react_prompt_message(sender, recipient, context):\n",
        "    return ReAct_prompt.format(input=context[\"question\"])\n",
        "\n",
        "with Cache.disk(cache_seed=5) as cache:\n",
        "  user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    question=\"What is LLAMA 3's release date?\",\n",
        "    max_turns=4,\n",
        "    message=react_prompt_message\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4NjoMUJlLzb",
        "outputId": "d30051ba-4375-4508-f40e-79b3b950f75e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to Assistant):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: What is LLAMA 3's release date?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to user_proxy):\n",
            "\n",
            "***** Suggested tool call (call_qlsJXfiDAbIYtsYuSwvjNxxB): search_tool *****\n",
            "Arguments: \n",
            "{\"query\":\"LLAMA 3 release date\"}\n",
            "****************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION search_tool...\n",
            "user_proxy (to Assistant):\n",
            "\n",
            "user_proxy (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_qlsJXfiDAbIYtsYuSwvjNxxB) *****\n",
            "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://ai.meta.com/blog/meta-llama-3/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The Llama 3 8B and 70B models mark the beginning of what we plan to release for Llama 3. And there's a lot more to come. Our largest models are over 400B parameters and, while these models are still training, our team is excited about how they're trending. ... The most capable openly available LLM to date. April 18, 2024. Takeaways ...\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://llama.meta.com/llama3/?ref=upstract.com\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"With the release of Llama 3, we've updated the Responsible Use Guide (RUG) to provide the most comprehensive information on responsible development with LLMs. ... Introducing Meta Llama 3: The most capable openly available LLM to date. Read the blog. Meet Your New Assistant: Meta AI, Built With Llama 3. Learn more. Meta Llama 3 repository ...\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.theguardian.com/technology/2024/apr/18/meta-ai-llama3-release?ref=upstract.com\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The social media giant equipped Llama 3 with new computer coding capabilities and fed it images as well as text this time, though for now the model will output only text, Chris Cox, Meta's chief ...\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.businessinsider.com/meta-releases-latest-ai-model-llama-3-2024-4?op=1\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Meta's large language model and AI assistant are getting upgrades. On Thursday, the company released the first models of Llama 3 in two sizes, 8B and 70B parameters. They've been integrated into ...\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://techcrunch.com/2024/04/18/meta-releases-llama-3-claims-its-among-the-best-open-models-available/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Meta has released several models in its new Llama 3 family, which it claims improve across the board in terms of performance versus Llama 2. ... with the rest to come at an unspecified future date ...\\\\\\\"}\\\"]\"\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to user_proxy):\n",
            "\n",
            "Observation: The release date for LLAMA 3 is April 18, 2024.\n",
            "Thought: I now know the final answer\n",
            "Final Answer: The release date for LLAMA 3 is April 18, 2024.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to Assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to user_proxy):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReAct with Teachability"
      ],
      "metadata": {
        "id": "QeBxRYKwybTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen.agentchat.contrib.capabilities import teachability\n",
        "\n",
        "# Instantiate Teachability capability\n",
        "teachability = teachability.Teachability(\n",
        "    verbosity=3,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
        "    reset_db=True,\n",
        "    path_to_db_dir=\"./teachability_db\",\n",
        "    recall_threshold=1.5,  # Squared L2, Higher numbers allow more (but less relevant) memos to be recalled.\n",
        ")\n",
        "\n",
        "# add Teachability capability\n",
        "teachability.add_to_agent(assistant)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whLQGBmCyZ9c",
        "outputId": "0c79e350-4443-4d1b-8dbf-7c7b156d1461"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CLEARING MEMORY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# starts with empty vector store\n",
        "\n",
        "with Cache.disk() as cache:\n",
        "  user_proxy.initiate_chat(\n",
        "      assistant,\n",
        "      question=\"What is LLAMA 3's release date?\",\n",
        "      max_turns=4,\n",
        "      message=react_prompt_message\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vMsJC5qn0wPW",
        "outputId": "b223a53f-f471-4de7-cee5-64df43174791"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to Assistant):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "analyzer (to Assistant):\n",
            "\n",
            "# INSTRUCTIONS\n",
            "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\n",
            "\n",
            "# TEXT\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "# INSTRUCTIONS\n",
            "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\n",
            "***** Suggested tool call (call_fbrGOE8fEdYHoHwdShoKiK95): search_tool *****\n",
            "Arguments: \n",
            "{\"query\":\"Does any part of the TEXT ask the agent to perform a task or solve a problem?\"}\n",
            "****************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "Briefly copy any advice from the TEXT that may be useful for a similar but different task in the future. But if no advice is present, just respond with 'none'.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "analyzer (to Assistant):\n",
            "\n",
            "none\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "Does the TEXT contain information that could be committed to memory? Answer with just one word, yes or no.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "analyzer (to Assistant):\n",
            "\n",
            "# INSTRUCTIONS\n",
            "Does the TEXT contain information that could be committed to memory? Answer with just one word, yes or no.\n",
            "\n",
            "# TEXT\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "# INSTRUCTIONS\n",
            "Does the TEXT contain information that could be committed to memory? Answer with just one word, yes or no.\n",
            "***** Suggested tool call (call_bHcVJ4sTBZ9HS0o5W44EUlkl): search_tool *****\n",
            "Arguments: \n",
            "{\"query\":\"Who is the current prime minister of Germany?\"}\n",
            "****************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "Imagine that the user forgot this information in the TEXT. How would they ask you for this information? Include no other text in your response.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "analyzer (to Assistant):\n",
            "\n",
            "Question: Who is the current prime minister of Germany?\n",
            "Thought: Think about how the user would ask for this information.\n",
            "Action: Search for the current prime minister of Germany.\n",
            "Action Input: \"current prime minister of Germany\"\n",
            "Observation: The search results display the name of the current chancellor of Germany.\n",
            "Thought: I now know the final answer\n",
            "Final Answer: The current chancellor of Germany is Angela Merkel.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "Copy the information from the TEXT that should be committed to memory. Add no explanation.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "analyzer (to Assistant):\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "REMEMBER THIS QUESTION-ANSWER PAIR\n",
            "\n",
            "INPUT-OUTPUT PAIR ADDED TO VECTOR DATABASE:\n",
            "  ID\n",
            "    1\n",
            "  INPUT\n",
            "    Question: Who is the current prime minister of Germany?\n",
            "Thought: Think about how the user would ask for this information.\n",
            "Action: Search for the current prime minister of Germany.\n",
            "Action Input: \"current prime minister of Germany\"\n",
            "Observation: The search results display the name of the current chancellor of Germany.\n",
            "Thought: I now know the final answer\n",
            "Final Answer: The current chancellor of Germany is Angela Merkel.\n",
            "  OUTPUT\n",
            "    Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "\n",
            "LIST OF MEMOS\n",
            "  ID: 1\n",
            "    INPUT TEXT: Question: Who is the current prime minister of Germany?\n",
            "Thought: Think about how the user would ask for this information.\n",
            "Action: Search for the current prime minister of Germany.\n",
            "Action Input: \"current prime minister of Germany\"\n",
            "Observation: The search results display the name of the current chancellor of Germany.\n",
            "Thought: I now know the final answer\n",
            "Final Answer: The current chancellor of Germany is Angela Merkel.\n",
            "    OUTPUT TEXT: Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "Assistant (to user_proxy):\n",
            "\n",
            "***** Suggested tool call (call_wBlU6N3hfx6sCqMocXKM6NGg): search_tool *****\n",
            "Arguments: \n",
            "{\"query\":\"current prime minister of Germany\"}\n",
            "****************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION search_tool...\n",
            "user_proxy (to Assistant):\n",
            "\n",
            "user_proxy (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_wBlU6N3hfx6sCqMocXKM6NGg) *****\n",
            "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.nytimes.com/live/2021/12/08/world/germany-scholz-merkel\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The oath for an incoming chancellor, as laid out in article 56 of Germany\\\\\\\\u2019s Constitution, reads as follows: \\\\\\\\u201cI swear that I will dedicate my strength to the good of the German people, that I will enhance their prosperity, avert harm from them, respect and defend the Constitution and the laws of the federal state, fulfill my duty conscientiously and exercise justice toward everyone. But one of Ms. Merkel\\\\\\\\u2019s choices intrigued many and set the German Twittersphere alight: \\\\\\\\u201cDu hast den Farbfilm vergessen,\\\\\\\\u201d or \\\\\\\\u201cYou Forgot the Color Film,\\\\\\\\u201d a 1970s hit from the Communist East by Nina Hagen, who later emigrated to the West and went on to become West Germany\\\\\\\\u2019s punk rock idol of the 1980s.\\\\\\\\n After announcing tough restrictions for unvaccinated people and an intention to make vaccinations mandatory, the incoming chancellor, Olaf Scholz, took the popular step this week of tapping Karl Lauterbach, a Harvard-trained public health expert and medical doctor, to run the health ministry.\\\\\\\\nMr. Lauterbach, often in a bow tie, has been a fixture on television debate shows and in newspaper interviews since the pandemic began. For the photographer Herlinde Koelbl, the end of Angela Merkel\\\\\\\\u2019s tenure as Germany\\\\\\\\u2019s chancellor isn\\\\\\\\u2019t just the end of a political era \\\\\\\\u2014 it\\\\\\\\u2019s also the end of one of the longest-running art projects in world politics.\\\\\\\\n In addition, a proposed law would require the installation of solar panels on the roofs of all new commercial buildings, with the amount of solar-powered energy in the country to be tripled by the end of this decade.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.npr.org/2021/12/08/1062130622/germany-new-chancellor-olaf-scholz-coalition-government\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"This Is The Candidate To Beat In The Race To Become Germany's Next Leader\\\\\\\\nOne of the reasons he and his party prevailed in the federal election is that in Scholz, Germans see a continuation of Merkel's governing style: Scholz is a calm, steady hand in a crisis and a pragmatic leader who strives for compromise.\\\\\\\\n World\\\\\\\\nWhat you need to know about Germany's new chancellor and coalition government\\\\\\\\nRob Schmitz\\\\\\\\nMembers of three German parties at the signing of a coalition agreement are (from left) Social Democrats Norbert Walter-Borjan, Saskia Esken and new Chancellor Olaf Scholz, Free Democrat Christian Lindner, and the Green party's Robert Habeck and Annalena Baerbock on Tuesday in Berlin.\\\\\\\\n \\\\\\\\\\\\\\\"We commit to a community of democracies across the globe and I am very grateful to President Biden for emphasizing the importance of multilateralism and at the same time, we are committed to what unites particular nations: the idea of freedom, the rule of law, democracy and respect for human rights. Andreas Gora/Pool/Getty Images\\\\\\\\nhide caption\\\\\\\\nClaudia Roth, designated minister of state for culture, Anne Spiegel, designated family affairs minister, Robert Habeck, designated minister of economy, energy and climate protection, Annalena Baerbock, designated foreign minister, Cem Ozdemir, designated agriculture minister, and Steffi Lemke, designated environment minister, present themselves to the media on Monday in Berlin.\\\\\\\\n Carsten Koall/Getty Images\\\\\\\\nhide caption\\\\\\\\nMembers of three German parties at the signing of a coalition agreement are (from left) Social Democrats Norbert Walter-Borjan, Saskia Esken and new Chancellor Olaf Scholz, Free Democrat Christian Lindner, and the Green party's Robert Habeck and Annalena Baerbock on Tuesday in Berlin.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.bbc.com/news/world-europe-59575773\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Ready for power: Team Scholz promises a new Germany\\\\\\\\nAfrican diaspora with 'a voice' in Germany\\\\\\\\nDefeated Merkel heir prepared to resign as leader\\\\\\\\nGermany lose in Austria as Nagelsmann woe continues\\\\\\\\nAustria hand Germany a second straight defeat as Julian Nagelsmann's reign continues to get off to a difficult start.\\\\\\\\n Nagelsmann loses on home debut as Germany boss\\\\\\\\nJulian Nagelsmann's home debut as Germany coach ends in defeat as the Euro 2024 hosts are beaten by Turkey in a friendly.\\\\\\\\nBerlin on edge for Erdogan after fierce Israel criticism\\\\\\\\nIsrael's war with Hamas takes centre stage as Turkey's leader meets Chancellor Olaf Scholz in Berlin.\\\\\\\\n Beta Terms By using the Beta Site, you agree that such use is at your own risk and you know that the Beta Site may include known or unknown bugs or errors, that we have no obligation to make this Beta Site available with or without charge for any period of time, nor to make it available at all, and that nothing in these Beta Terms or your use of the Beta Site creates any employment relationship between you and us. Although Russia has denied plans to invade its neighbour, Angela Merkel agreed with US President Joe Biden and the leaders of the UK, France and Italy late on Tuesday that they would adopt a joint strategy to respond by imposing \\\\\\\\\\\\\\\"significant and severe harm on the Russian economy\\\\\\\\\\\\\\\".\\\\\\\\n Mr Scholz, a soft-spoken 63-year-old, steered the Social Democrats to election victory in late September, positioning himself as the continuity candidate because he played a key role in the Merkel government as vice-chancellor.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://en.wikipedia.org/wiki/Olaf_Scholz\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"On 10 August 2020, SPD party leadership agreed that it would nominate Scholz to be the party's candidate for Chancellor of Germany at the 2021 federal election.[105] Scholz is usually grouped with the moderate wing of the SPD,[106] and his nomination was seen by Die Tageszeitung as marking a decline of the party's left.[107]\\\\\\\\nScholz led the SPD to a narrow victory in the election, with the party receiving 25.8% of the second votes and 206 seats in the Bundestag.[108] Following this victory, Scholz was widely considered to be the most likely next Chancellor of Germany in a traffic light coalition with The Greens and the Free Democratic Party.[109]\\\\\\\\nOn 24 November, the SPD, Green and FDP reached a coalition agreement, naming Scholz as the new German chancellor.[110]\\\\\\\\nChancellor of Germany, 2021\\\\\\\\u2013present[edit]\\\\\\\\nScholz was elected Chancellor by the Bundestag on 8 December 2021, with 395 votes in favour and 303 against.[111] Also in June 2021, Scholz oversaw the Federal Central Tax Office's purchase of information regarding German citizens using Dubai for tax avoidance and evasion.[58]\\\\\\\\nScholz was criticized in the context of the Wirecard scandal; serious misconduct by the Federal Financial Supervisory Authority (BaFin), which is under the responsibility of the Federal Ministry of Finance, is alleged to have contributed to the longevity of the fraudulent business.[59][60] During Scholz's time in office, the Ministry of Finance was one of the subjects of parliamentary inquiry into the scandal, but Scholz has denied any personal responsibility.[61][62] Journalist Hermann-Josef Tenhagen criticized this version of the transaction tax on the basis that it would disproportionately affect lower-income individuals.[77] A report by the Kiel Institute for the World Economy commissioned by the Federal Government in 2020 certified the same deficiencies in the tax concept that Tenhagen had already pointed out.[78]\\\\\\\\nDuring his tenure as minister of finance, Scholz prioritized not taking on new government debt and limiting public spending.[50] In 2018, he suggested the creation of an EU-wide unemployment insurance system to make the Eurozone more resilient to future economic shocks.[79]\\\\\\\\nIn September 2019, Scholz negotiated the climate package in a key role for the SPD. Alongside fellow Social Democrats J\\\\\\\\u00f6rg Asmussen and Thomas Oppermann, Scholz was reported in the media to be a possible successor to Sch\\\\\\\\u00e4uble in the post of Finance Minister at the time; whilst Sch\\\\\\\\u00e4uble remained in post, the talks to form a coalition were ultimately successful.[39]\\\\\\\\nIn a paper compiled in late 2014, Scholz and Sch\\\\\\\\u00e4uble proposed redirecting revenue from the solidarity surcharge on income and corporate tax (Solidarit\\\\\\\\u00e4tszuschlag) to subsidize the federal states' interest payments.[40]\\\\\\\\nUnder Scholz's leadership, the Social Democrats won the 2015 state election in Hamburg, receiving around 47% of the vote.[41] In January 2022, The New York Times reported intensifying concerns from the US and other NATO allies about the Scholz government's \\\\\\\\\\\\\\\"evident hesitation to take forceful measures\\\\\\\\\\\\\\\" against Russia in the 2021\\\\\\\\u20132022 Russo-Ukrainian crisis.[149]\\\\\\\\nThe Scholz government initially refused to send weapons to Ukraine, citing existing German policy and financial support for the Eastern European country.[150] As late as 15 February, Scholz was quoted by TASS as saying \\\\\\\\\\\\\\\"the way out of the crisis in Ukraine is to implement the Steinmeier formula\\\\\\\\\\\\\\\", a mechanism of granting a special status to Donbass.[151]\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.cnn.com/2021/12/08/europe/germany-olaf-scholz-chancellor-inauguration-intl/index.html\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"CNN values your feedback\\\\\\\\nOlaf Scholz appointed as Germany\\\\\\\\u2019s new chancellor, replacing Angela Merkel after 16 years\\\\\\\\nOlaf Scholz has been sworn in as Germany\\\\\\\\u2019s new Chancellor on Wednesday, bringing to an end Angela Merkel\\\\\\\\u2019s four terms at the helm of Europe\\\\\\\\u2019s largest economy.\\\\\\\\nScholz, the leader of the Social Democratic Party (SPD), won the secret vote in the Parliament as expected, a culmination of months of negotiations following the SPD\\\\\\\\u2019s narrow victory in September\\\\\\\\u2019s federal elections.\\\\\\\\n The Greens will be taking over the foreign ministry, the environment ministry and the newly created ministry of the economy and climate, while the FDP will be in charge of the finance ministry, the justice department and the education ministry.\\\\\\\\n \\\\\\\\u201cScholz seems to partly owe his success [in the election] to posing as [Merkel\\\\\\\\u2019s] worthy heir during the campaign, calm and unassuming \\\\\\\\u2013 and with his hands folded into a Merkel-style rhombus in a picture that went viral,\\\\\\\\u201d Holger Schmieding, the chief economist at Berenberg Bank, wrote in an analyst note on Wednesday. Merkel, who watched the parliamentary proceedings from the visitors\\\\\\\\u2019 gallery alongside former chancellor Gerhard Schroeder, received applause from lawmakers when name-checked by the parliamentary president Baerbel Bas.\\\\\\\\nHaving led Germany for 16 years and 16 days, Merkel has narrowly missed on becoming the longest serving post-war Chancellor, trailing Helmut Kohl by mere 10 days.\\\\\\\\n The 63-year-old life-long member of the SPD served as the Labor and Social Affairs minister in Merkel\\\\\\\\u2019s first coalition government in the late 2000s.\\\\\\\"}\\\"]\"\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "LOOK FOR RELEVANT MEMOS, AS QUESTION-ANSWER PAIRS\n",
            "\n",
            "INPUT-OUTPUT PAIR RETRIEVED FROM VECTOR DATABASE:\n",
            "  INPUT1\n",
            "    Question: Who is the current prime minister of Germany?\n",
            "Thought: Think about how the user would ask for this information.\n",
            "Action: Search for the current prime minister of Germany.\n",
            "Action Input: \"current prime minister of Germany\"\n",
            "Observation: The search results display the name of the current chancellor of Germany.\n",
            "Thought: I now know the final answer\n",
            "Final Answer: The current chancellor of Germany is Angela Merkel.\n",
            "  OUTPUT\n",
            "    Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: Who is the current prime minister of germany?\n",
            "  DISTANCE\n",
            "    1.256562948506396\n",
            "Assistant (to analyzer):\n",
            "\n",
            "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.nytimes.com/live/2021/12/08/world/germany-scholz-merkel\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The oath for an incoming chancellor, as laid out in article 56 of Germany\\\\\\\\u2019s Constitution, reads as follows: \\\\\\\\u201cI swear that I will dedicate my strength to the good of the German people, that I will enhance their prosperity, avert harm from them, respect and defend the Constitution and the laws of the federal state, fulfill my duty conscientiously and exercise justice toward everyone. But one of Ms. Merkel\\\\\\\\u2019s choices intrigued many and set the German Twittersphere alight: \\\\\\\\u201cDu hast den Farbfilm vergessen,\\\\\\\\u201d or \\\\\\\\u201cYou Forgot the Color Film,\\\\\\\\u201d a 1970s hit from the Communist East by Nina Hagen, who later emigrated to the West and went on to become West Germany\\\\\\\\u2019s punk rock idol of the 1980s.\\\\\\\\n After announcing tough restrictions for unvaccinated people and an intention to make vaccinations mandatory, the incoming chancellor, Olaf Scholz, took the popular step this week of tapping Karl Lauterbach, a Harvard-trained public health expert and medical doctor, to run the health ministry.\\\\\\\\nMr. Lauterbach, often in a bow tie, has been a fixture on television debate shows and in newspaper interviews since the pandemic began. For the photographer Herlinde Koelbl, the end of Angela Merkel\\\\\\\\u2019s tenure as Germany\\\\\\\\u2019s chancellor isn\\\\\\\\u2019t just the end of a political era \\\\\\\\u2014 it\\\\\\\\u2019s also the end of one of the longest-running art projects in world politics.\\\\\\\\n In addition, a proposed law would require the installation of solar panels on the roofs of all new commercial buildings, with the amount of solar-powered energy in the country to be tripled by the end of this decade.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.npr.org/2021/12/08/1062130622/germany-new-chancellor-olaf-scholz-coalition-government\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"This Is The Candidate To Beat In The Race To Become Germany's Next Leader\\\\\\\\nOne of the reasons he and his party prevailed in the federal election is that in Scholz, Germans see a continuation of Merkel's governing style: Scholz is a calm, steady hand in a crisis and a pragmatic leader who strives for compromise.\\\\\\\\n World\\\\\\\\nWhat you need to know about Germany's new chancellor and coalition government\\\\\\\\nRob Schmitz\\\\\\\\nMembers of three German parties at the signing of a coalition agreement are (from left) Social Democrats Norbert Walter-Borjan, Saskia Esken and new Chancellor Olaf Scholz, Free Democrat Christian Lindner, and the Green party's Robert Habeck and Annalena Baerbock on Tuesday in Berlin.\\\\\\\\n \\\\\\\\\\\\\\\"We commit to a community of democracies across the globe and I am very grateful to President Biden for emphasizing the importance of multilateralism and at the same time, we are committed to what unites particular nations: the idea of freedom, the rule of law, democracy and respect for human rights. Andreas Gora/Pool/Getty Images\\\\\\\\nhide caption\\\\\\\\nClaudia Roth, designated minister of state for culture, Anne Spiegel, designated family affairs minister, Robert Habeck, designated minister of economy, energy and climate protection, Annalena Baerbock, designated foreign minister, Cem Ozdemir, designated agriculture minister, and Steffi Lemke, designated environment minister, present themselves to the media on Monday in Berlin.\\\\\\\\n Carsten Koall/Getty Images\\\\\\\\nhide caption\\\\\\\\nMembers of three German parties at the signing of a coalition agreement are (from left) Social Democrats Norbert Walter-Borjan, Saskia Esken and new Chancellor Olaf Scholz, Free Democrat Christian Lindner, and the Green party's Robert Habeck and Annalena Baerbock on Tuesday in Berlin.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.bbc.com/news/world-europe-59575773\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Ready for power: Team Scholz promises a new Germany\\\\\\\\nAfrican diaspora with 'a voice' in Germany\\\\\\\\nDefeated Merkel heir prepared to resign as leader\\\\\\\\nGermany lose in Austria as Nagelsmann woe continues\\\\\\\\nAustria hand Germany a second straight defeat as Julian Nagelsmann's reign continues to get off to a difficult start.\\\\\\\\n Nagelsmann loses on home debut as Germany boss\\\\\\\\nJulian Nagelsmann's home debut as Germany coach ends in defeat as the Euro 2024 hosts are beaten by Turkey in a friendly.\\\\\\\\nBerlin on edge for Erdogan after fierce Israel criticism\\\\\\\\nIsrael's war with Hamas takes centre stage as Turkey's leader meets Chancellor Olaf Scholz in Berlin.\\\\\\\\n Beta Terms By using the Beta Site, you agree that such use is at your own risk and you know that the Beta Site may include known or unknown bugs or errors, that we have no obligation to make this Beta Site available with or without charge for any period of time, nor to make it available at all, and that nothing in these Beta Terms or your use of the Beta Site creates any employment relationship between you and us. Although Russia has denied plans to invade its neighbour, Angela Merkel agreed with US President Joe Biden and the leaders of the UK, France and Italy late on Tuesday that they would adopt a joint strategy to respond by imposing \\\\\\\\\\\\\\\"significant and severe harm on the Russian economy\\\\\\\\\\\\\\\".\\\\\\\\n Mr Scholz, a soft-spoken 63-year-old, steered the Social Democrats to election victory in late September, positioning himself as the continuity candidate because he played a key role in the Merkel government as vice-chancellor.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://en.wikipedia.org/wiki/Olaf_Scholz\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"On 10 August 2020, SPD party leadership agreed that it would nominate Scholz to be the party's candidate for Chancellor of Germany at the 2021 federal election.[105] Scholz is usually grouped with the moderate wing of the SPD,[106] and his nomination was seen by Die Tageszeitung as marking a decline of the party's left.[107]\\\\\\\\nScholz led the SPD to a narrow victory in the election, with the party receiving 25.8% of the second votes and 206 seats in the Bundestag.[108] Following this victory, Scholz was widely considered to be the most likely next Chancellor of Germany in a traffic light coalition with The Greens and the Free Democratic Party.[109]\\\\\\\\nOn 24 November, the SPD, Green and FDP reached a coalition agreement, naming Scholz as the new German chancellor.[110]\\\\\\\\nChancellor of Germany, 2021\\\\\\\\u2013present[edit]\\\\\\\\nScholz was elected Chancellor by the Bundestag on 8 December 2021, with 395 votes in favour and 303 against.[111] Also in June 2021, Scholz oversaw the Federal Central Tax Office's purchase of information regarding German citizens using Dubai for tax avoidance and evasion.[58]\\\\\\\\nScholz was criticized in the context of the Wirecard scandal; serious misconduct by the Federal Financial Supervisory Authority (BaFin), which is under the responsibility of the Federal Ministry of Finance, is alleged to have contributed to the longevity of the fraudulent business.[59][60] During Scholz's time in office, the Ministry of Finance was one of the subjects of parliamentary inquiry into the scandal, but Scholz has denied any personal responsibility.[61][62] Journalist Hermann-Josef Tenhagen criticized this version of the transaction tax on the basis that it would disproportionately affect lower-income individuals.[77] A report by the Kiel Institute for the World Economy commissioned by the Federal Government in 2020 certified the same deficiencies in the tax concept that Tenhagen had already pointed out.[78]\\\\\\\\nDuring his tenure as minister of finance, Scholz prioritized not taking on new government debt and limiting public spending.[50] In 2018, he suggested the creation of an EU-wide unemployment insurance system to make the Eurozone more resilient to future economic shocks.[79]\\\\\\\\nIn September 2019, Scholz negotiated the climate package in a key role for the SPD. Alongside fellow Social Democrats J\\\\\\\\u00f6rg Asmussen and Thomas Oppermann, Scholz was reported in the media to be a possible successor to Sch\\\\\\\\u00e4uble in the post of Finance Minister at the time; whilst Sch\\\\\\\\u00e4uble remained in post, the talks to form a coalition were ultimately successful.[39]\\\\\\\\nIn a paper compiled in late 2014, Scholz and Sch\\\\\\\\u00e4uble proposed redirecting revenue from the solidarity surcharge on income and corporate tax (Solidarit\\\\\\\\u00e4tszuschlag) to subsidize the federal states' interest payments.[40]\\\\\\\\nUnder Scholz's leadership, the Social Democrats won the 2015 state election in Hamburg, receiving around 47% of the vote.[41] In January 2022, The New York Times reported intensifying concerns from the US and other NATO allies about the Scholz government's \\\\\\\\\\\\\\\"evident hesitation to take forceful measures\\\\\\\\\\\\\\\" against Russia in the 2021\\\\\\\\u20132022 Russo-Ukrainian crisis.[149]\\\\\\\\nThe Scholz government initially refused to send weapons to Ukraine, citing existing German policy and financial support for the Eastern European country.[150] As late as 15 February, Scholz was quoted by TASS as saying \\\\\\\\\\\\\\\"the way out of the crisis in Ukraine is to implement the Steinmeier formula\\\\\\\\\\\\\\\", a mechanism of granting a special status to Donbass.[151]\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.cnn.com/2021/12/08/europe/germany-olaf-scholz-chancellor-inauguration-intl/index.html\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"CNN values your feedback\\\\\\\\nOlaf Scholz appointed as Germany\\\\\\\\u2019s new chancellor, replacing Angela Merkel after 16 years\\\\\\\\nOlaf Scholz has been sworn in as Germany\\\\\\\\u2019s new Chancellor on Wednesday, bringing to an end Angela Merkel\\\\\\\\u2019s four terms at the helm of Europe\\\\\\\\u2019s largest economy.\\\\\\\\nScholz, the leader of the Social Democratic Party (SPD), won the secret vote in the Parliament as expected, a culmination of months of negotiations following the SPD\\\\\\\\u2019s narrow victory in September\\\\\\\\u2019s federal elections.\\\\\\\\n The Greens will be taking over the foreign ministry, the environment ministry and the newly created ministry of the economy and climate, while the FDP will be in charge of the finance ministry, the justice department and the education ministry.\\\\\\\\n \\\\\\\\u201cScholz seems to partly owe his success [in the election] to posing as [Merkel\\\\\\\\u2019s] worthy heir during the campaign, calm and unassuming \\\\\\\\u2013 and with his hands folded into a Merkel-style rhombus in a picture that went viral,\\\\\\\\u201d Holger Schmieding, the chief economist at Berenberg Bank, wrote in an analyst note on Wednesday. Merkel, who watched the parliamentary proceedings from the visitors\\\\\\\\u2019 gallery alongside former chancellor Gerhard Schroeder, received applause from lawmakers when name-checked by the parliamentary president Baerbel Bas.\\\\\\\\nHaving led Germany for 16 years and 16 days, Merkel has narrowly missed on becoming the longest serving post-war Chancellor, trailing Helmut Kohl by mere 10 days.\\\\\\\\n The 63-year-old life-long member of the SPD served as the Labor and Social Affairs minister in Merkel\\\\\\\\u2019s first coalition government in the late 2000s.\\\\\\\"}\\\"]\"\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to analyzer):\n",
            "\n",
            "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d1fc12781125>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   user_proxy.initiate_chat(\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0massistant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Who is the current prime minister of germany?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36minitiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmsg2send\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg2send\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreply_at_receive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m         \u001b[0;31m# Call the hookable method that gives registered hooks a chance to process the last message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \u001b[0;31m# Message modifications do not affect the incoming messages or self._oai_messages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1908\u001b[0;31m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_last_received_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;31m# Call the hookable method that gives registered hooks a chance to process all messages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mprocess_last_received_message\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mprocessed_user_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhook_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2691\u001b[0;31m             \u001b[0mprocessed_user_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_user_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessed_user_content\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser_content\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2693\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmessages\u001b[0m  \u001b[0;31m# No hooks actually modified the user's message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/contrib/capabilities/teachability.py\u001b[0m in \u001b[0;36mprocess_last_received_message\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mexpanded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemo_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_memo_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mexpanded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consider_memo_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Try to store any user teachings in new memos to be used in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/contrib/capabilities/teachability.py\u001b[0m in \u001b[0;36m_consider_memo_retrieval\u001b[0;34m(self, comment)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Next, if the comment involves a task, then extract and generalize the task before using it as the lookup key.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         response = self._analyze(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mcomment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;34m\"Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/contrib/capabilities/teachability.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(self, text_to_analyze, analysis_instructions)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mrecipient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_analyze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         )  # Put the message in the analyzer's list.\n\u001b[0;32m--> 234\u001b[0;31m         self.teachable_agent.send(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mrecipient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalysis_instructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         )  # Request the reply.\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreply_at_receive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1919\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trigger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m                 \u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/contrib/text_analyzer_agent.py\u001b[0m in \u001b[0;36m_analyze_in_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Delegate to the analysis method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_to_analyze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_instructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/contrib/text_analyzer_agent.py\u001b[0m in \u001b[0;36manalyze_text\u001b[0;34m(self, text_to_analyze, analysis_instructions)\u001b[0m\n\u001b[1;32m     69\u001b[0m         )  # Repeat the instructions.\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Generate and return the analysis string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_oai_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmsg_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oai_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         extracted_response = self._generate_oai_reply_from_client(\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oai_system_message\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36m_generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;31m# TODO: #1143 handle token limit exceeded error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         response = llm_client.create(\n\u001b[0m\u001b[1;32m   1307\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m                 \u001b[0mrequest_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAPITimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"config {i} timed out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 581\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         )\n\u001b[0;32m-> 1232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 921\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    951\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    231\u001b[0m         )\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    114\u001b[0m                 trace.return_value = (\n\u001b[1;32m    115\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful info stored in vector store now\n",
        "with Cache.disk() as cache:\n",
        "  user_proxy.initiate_chat(\n",
        "      assistant,\n",
        "      question=\"Who is the current prime minister of germany?\",\n",
        "      max_turns=4,\n",
        "      message=react_prompt_message\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ujLZxr0wjt",
        "outputId": "91100b36-6322-4de5-c2e0-e5a16fcd38a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to Assistant):\n",
            "\n",
            "\n",
            "Answer the following questions as best you can. You have access to tools provided.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this process can repeat multiple times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "Question: What is the result of super bowl 2024?\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to user_proxy):\n",
            "\n",
            "Question: What is the result of super bowl 2024?\n",
            "Thought: Based on the information provided, the Kansas City Chiefs won Super Bowl 2024.\n",
            "Final Answer: The Kansas City Chiefs won Super Bowl 2024.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to Assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Assistant (to user_proxy):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from autogen.agentchat.contrib.capabilities.teachability import MemoStore\n",
        "\n",
        "# memo_store = MemoStore(verbosity=0, reset=False, path_to_db_dir='./teachability_db')\n",
        "# memo_store.list_memos()"
      ],
      "metadata": {
        "id": "74RoJWnA7tqo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "YN5CcbUnAjr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markdownify sentence_transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6VNRAc8E4HS",
        "outputId": "a64508df-3844-4ff3-cf39-1889ae2b3a29"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from google.colab import userdata\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 1000,\n",
        "}\n",
        "\n",
        "retrive_config = {\n",
        "    \"task\": \"qa\",\n",
        "    \"docs_path\": [\n",
        "        \"https://raw.githubusercontent.com/shah-zeb-naveed/autogen-udemy-course/main/marsium_republic.txt\",\n",
        "    ],\n",
        "    \"chunk_token_size\": 50,\n",
        "    \"model\": llm_config[\"model\"],\n",
        "    \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
        "    \"embedding_model\": \"all-mpnet-base-v2\",\n",
        "    \"get_or_create\": True,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
        "}\n",
        "\n",
        "rag_agent = RetrieveUserProxyAgent(\n",
        "    name=\"rag_agent\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    retrieve_config=retrive_config,\n",
        "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
        ")\n",
        "\n",
        "assistant = RetrieveAssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    system_message=\"You are a helpful AI assistant.\",\n",
        "    llm_config={\n",
        "        \"timeout\": 600,\n",
        "        \"config_list\": [llm_config],\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "KemopNCJAmRu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"On what planet is Marsium Republic situated?\"\n",
        "\n",
        "with Cache.disk() as cache:\n",
        "  rag_agent.initiate_chat(\n",
        "      assistant,\n",
        "      problem=query,\n",
        "      message=rag_agent.message_generator,\n",
        "      max_turns=1\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn85DXZBEtnb",
        "outputId": "a3d896e7-01f5-4ff3-fbad-2149a5082921"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t﻿Title: Marsium Republic ...\n",
            "WARNING:autogen.retrieve_utils:Failed to split docs with must_break_at_empty_line being True, set to False.\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe Marsium Republic is a hypothetical nation situated on the red planet, Mars. It's capital is Olym ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tarsium Republic is a hypothetical nation situated on the red planet, Mars. It's capital is Olympus.  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tm Republic is a hypothetical nation situated on the red planet, Mars. It's capital is Olympus. Found ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tublic is a hypothetical nation situated on the red planet, Mars. It's capital is Olympus. Founded in ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t is a hypothetical nation situated on the red planet, Mars. It's capital is Olympus. Founded in the  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t hypothetical nation situated on the red planet, Mars. It's capital is Olympus. Founded in the 22nd  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tthetical nation situated on the red planet, Mars. It's capital is Olympus. Founded in the 22nd centu ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcal nation situated on the red planet, Mars. It's capital is Olympus. Founded in the 22nd century by ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tation situated on the red planet, Mars. It's capital is Olympus. Founded in the 22nd century by a co ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t situated on the red planet, Mars. It's capital is Olympus. Founded in the 22nd century by a coaliti ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to create collection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tated on the red planet, Mars. It's capital is Olympus. Founded in the 22nd century by a coalition of ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ton the red planet, Mars. It's capital is Olympus. Founded in the 22nd century by a coalition of inte ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te red planet, Mars. It's capital is Olympus. Founded in the 22nd century by a coalition of internati ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t planet, Mars. It's capital is Olympus. Founded in the 22nd century by a coalition of international  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tet, Mars. It's capital is Olympus. Founded in the 22nd century by a coalition of international space ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tars. It's capital is Olympus. Founded in the 22nd century by a coalition of international space agen ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tIt's capital is Olympus. Founded in the 22nd century by a coalition of international space agencies  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcapital is Olympus. Founded in the 22nd century by a coalition of international space agencies and p ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tal is Olympus. Founded in the 22nd century by a coalition of international space agencies and pionee ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Olympus. Founded in the 22nd century by a coalition of international space agencies and pioneering  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpus. Founded in the 22nd century by a coalition of international space agencies and pioneering settl ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tFounded in the 22nd century by a coalition of international space agencies and pioneering settlers,  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted in the 22nd century by a coalition of international space agencies and pioneering settlers, Marsi ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t the 22nd century by a coalition of international space agencies and pioneering settlers, Marsium Re ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe genesis of the Marsium Republic traces back to the early 21st century when scientists and vision ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tenesis of the Marsium Republic traces back to the early 21st century when scientists and visionaries ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts of the Marsium Republic traces back to the early 21st century when scientists and visionaries bega ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tthe Marsium Republic traces back to the early 21st century when scientists and visionaries began con ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tarsium Republic traces back to the early 21st century when scientists and visionaries began contempl ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tm Republic traces back to the early 21st century when scientists and visionaries began contemplating ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tublic traces back to the early 21st century when scientists and visionaries began contemplating the  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t traces back to the early 21st century when scientists and visionaries began contemplating the colon ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tes back to the early 21st century when scientists and visionaries began contemplating the colonizati ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tck to the early 21st century when scientists and visionaries began contemplating the colonization of ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t the early 21st century when scientists and visionaries began contemplating the colonization of Mars ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tearly 21st century when scientists and visionaries began contemplating the colonization of Mars as h ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t 21st century when scientists and visionaries began contemplating the colonization of Mars as humani ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t century when scientists and visionaries began contemplating the colonization of Mars as humanity's  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tury when scientists and visionaries began contemplating the colonization of Mars as humanity's next  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tIn a landmark agreement, leading spacefaring nations collaboratively formed the Marsium Republic, tr ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlandmark agreement, leading spacefaring nations collaboratively formed the Marsium Republic, transce ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tark agreement, leading spacefaring nations collaboratively formed the Marsium Republic, transcending ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tgreement, leading spacefaring nations collaboratively formed the Marsium Republic, transcending geop ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tent, leading spacefaring nations collaboratively formed the Marsium Republic, transcending geopoliti ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tleading spacefaring nations collaboratively formed the Marsium Republic, transcending geopolitical b ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tng spacefaring nations collaboratively formed the Marsium Republic, transcending geopolitical bounda ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tacefaring nations collaboratively formed the Marsium Republic, transcending geopolitical boundaries  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe territory of the Marsium Republic encompasses diverse Martian landscapes, ranging from expansive ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\territory of the Marsium Republic encompasses diverse Martian landscapes, ranging from expansive dese ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tory of the Marsium Republic encompasses diverse Martian landscapes, ranging from expansive deserts a ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tf the Marsium Republic encompasses diverse Martian landscapes, ranging from expansive deserts and to ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Marsium Republic encompasses diverse Martian landscapes, ranging from expansive deserts and towerin ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tium Republic encompasses diverse Martian landscapes, ranging from expansive deserts and towering vol ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tepublic encompasses diverse Martian landscapes, ranging from expansive deserts and towering volcanoe ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tic encompasses diverse Martian landscapes, ranging from expansive deserts and towering volcanoes to  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcompasses diverse Martian landscapes, ranging from expansive deserts and towering volcanoes to icy p ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsses diverse Martian landscapes, ranging from expansive deserts and towering volcanoes to icy polar  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tdiverse Martian landscapes, ranging from expansive deserts and towering volcanoes to icy polar regio ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tse Martian landscapes, ranging from expansive deserts and towering volcanoes to icy polar regions an ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trtian landscapes, ranging from expansive deserts and towering volcanoes to icy polar regions and anc ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t landscapes, ranging from expansive deserts and towering volcanoes to icy polar regions and ancient  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tscapes, ranging from expansive deserts and towering volcanoes to icy polar regions and ancient river ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts, ranging from expansive deserts and towering volcanoes to icy polar regions and ancient riverbeds. ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnging from expansive deserts and towering volcanoes to icy polar regions and ancient riverbeds. Key  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t from expansive deserts and towering volcanoes to icy polar regions and ancient riverbeds. Key settl ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t expansive deserts and towering volcanoes to icy polar regions and ancient riverbeds. Key settlement ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnsive deserts and towering volcanoes to icy polar regions and ancient riverbeds. Key settlements inc ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t deserts and towering volcanoes to icy polar regions and ancient riverbeds. Key settlements include  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trts and towering volcanoes to icy polar regions and ancient riverbeds. Key settlements include the c ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnd towering volcanoes to icy polar regions and ancient riverbeds. Key settlements include the capita ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\twering volcanoes to icy polar regions and ancient riverbeds. Key settlements include the capital cit ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tg volcanoes to icy polar regions and ancient riverbeds. Key settlements include the capital city of  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tMarsium Republic's geography is characterized by ongoing terraforming efforts aimed at enhancing hab ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tum Republic's geography is characterized by ongoing terraforming efforts aimed at enhancing habitabi ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tArt, literature, and music flourish in the Martian cultural landscape, inspired by the awe-inspiring ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tliterature, and music flourish in the Martian cultural landscape, inspired by the awe-inspiring vist ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tature, and music flourish in the Martian cultural landscape, inspired by the awe-inspiring vistas of ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t, and music flourish in the Martian cultural landscape, inspired by the awe-inspiring vistas of the  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t music flourish in the Martian cultural landscape, inspired by the awe-inspiring vistas of the Marti ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tc flourish in the Martian cultural landscape, inspired by the awe-inspiring vistas of the Martian fr ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\turish in the Martian cultural landscape, inspired by the awe-inspiring vistas of the Martian frontie ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t in the Martian cultural landscape, inspired by the awe-inspiring vistas of the Martian frontier and ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\the Martian cultural landscape, inspired by the awe-inspiring vistas of the Martian frontier and the  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trtian cultural landscape, inspired by the awe-inspiring vistas of the Martian frontier and the resil ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t cultural landscape, inspired by the awe-inspiring vistas of the Martian frontier and the resilience ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tural landscape, inspired by the awe-inspiring vistas of the Martian frontier and the resilience of t ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlandscape, inspired by the awe-inspiring vistas of the Martian frontier and the resilience of the hu ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcape, inspired by the awe-inspiring vistas of the Martian frontier and the resilience of the human s ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe Marsium Republic operates under a democratic system of governance, with elected representatives  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tarsium Republic operates under a democratic system of governance, with elected representatives respo ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tm Republic operates under a democratic system of governance, with elected representatives responsibl ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tublic operates under a democratic system of governance, with elected representatives responsible for ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe Martian Parliament, comprised of elected senators representing Martian settlements and districts ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tartian Parliament, comprised of elected senators representing Martian settlements and districts, del ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn Parliament, comprised of elected senators representing Martian settlements and districts, delibera ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tliament, comprised of elected senators representing Martian settlements and districts, deliberates o ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnt, comprised of elected senators representing Martian settlements and districts, deliberates on leg ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tomprised of elected senators representing Martian settlements and districts, deliberates on legislat ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsed of elected senators representing Martian settlements and districts, deliberates on legislative m ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tf elected senators representing Martian settlements and districts, deliberates on legislative matter ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcted senators representing Martian settlements and districts, deliberates on legislative matters and ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsenators representing Martian settlements and districts, deliberates on legislative matters and form ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tors representing Martian settlements and districts, deliberates on legislative matters and formulate ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tepresenting Martian settlements and districts, deliberates on legislative matters and formulates pol ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tenting Martian settlements and districts, deliberates on legislative matters and formulates policies ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tg Martian settlements and districts, deliberates on legislative matters and formulates policies to p ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ttian settlements and districts, deliberates on legislative matters and formulates policies to promot ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsettlements and districts, deliberates on legislative matters and formulates policies to promote the ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tements and districts, deliberates on legislative matters and formulates policies to promote the welf ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts and districts, deliberates on legislative matters and formulates policies to promote the welfare a ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe Marsium Republic stands as a testament to humanity's indomitable spirit of exploration, cooperat ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tarsium Republic stands as a testament to humanity's indomitable spirit of exploration, cooperation,  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tm Republic stands as a testament to humanity's indomitable spirit of exploration, cooperation, and i ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tublic stands as a testament to humanity's indomitable spirit of exploration, cooperation, and innova ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t stands as a testament to humanity's indomitable spirit of exploration, cooperation, and innovation. ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tds as a testament to humanity's indomitable spirit of exploration, cooperation, and innovation. As m ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t a testament to humanity's indomitable spirit of exploration, cooperation, and innovation. As mankin ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tstament to humanity's indomitable spirit of exploration, cooperation, and innovation. As mankind con ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnt to humanity's indomitable spirit of exploration, cooperation, and innovation. As mankind continue ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t humanity's indomitable spirit of exploration, cooperation, and innovation. As mankind continues its ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnity's indomitable spirit of exploration, cooperation, and innovation. As mankind continues its jour ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts indomitable spirit of exploration, cooperation, and innovation. As mankind continues its journey i ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tomitable spirit of exploration, cooperation, and innovation. As mankind continues its journey into t ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tble spirit of exploration, cooperation, and innovation. As mankind continues its journey into the co ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpirit of exploration, cooperation, and innovation. As mankind continues its journey into the cosmos, ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t of exploration, cooperation, and innovation. As mankind continues its journey into the cosmos, the  ...\n",
            "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\txploration, cooperation, and innovation. As mankind continues its journey into the cosmos, the Marsi ...\n",
            "INFO:autogen.retrieve_utils:Found 144 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc_ids:  [['doc_0', 'doc_26', 'doc_143', 'doc_58', 'doc_52', 'doc_84', 'doc_124', 'doc_80', 'doc_105', 'doc_43', 'doc_82', 'doc_79', 'doc_99', 'doc_46', 'doc_59', 'doc_97', 'doc_67', 'doc_107', 'doc_94', 'doc_66']]\n",
            "Adding doc_id doc_0 to context.\n",
            "Adding doc_id doc_26 to context.\n",
            "Adding doc_id doc_143 to context.\n",
            "Adding doc_id doc_58 to context.\n",
            "Adding doc_id doc_52 to context.\n",
            "Adding doc_id doc_84 to context.\n",
            "Adding doc_id doc_124 to context.\n",
            "Adding doc_id doc_80 to context.\n",
            "Adding doc_id doc_105 to context.\n",
            "Adding doc_id doc_43 to context.\n",
            "Adding doc_id doc_82 to context.\n",
            "Adding doc_id doc_79 to context.\n",
            "Adding doc_id doc_99 to context.\n",
            "Adding doc_id doc_46 to context.\n",
            "Adding doc_id doc_59 to context.\n",
            "Adding doc_id doc_97 to context.\n",
            "Adding doc_id doc_67 to context.\n",
            "Adding doc_id doc_107 to context.\n",
            "Adding doc_id doc_94 to context.\n",
            "Adding doc_id doc_66 to context.\n",
            "rag_agent (to assistant):\n",
            "\n",
            "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
            "context provided by the user.\n",
            "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
            "You must give as short an answer as possible.\n",
            "\n",
            "User's question is: On what planet is Marsium Republic situated?\n",
            "\n",
            "Context is: ﻿Title: Marsium Republic\n",
            "22nd century by a coalition of international space agencies and pioneering settlers, Marsium Republic represents a bold leap in human civilization's expansion beyond Earth's confines. This article provides an overview of the country's history, geography, culture, and governance.\n",
            "ation, cooperation, and innovation. As mankind continues its journey into the cosmos, the Marsium Republic serves as a beacon of hope and inspiration, demonstrating the boundless potential of human civilization to transcend earthly limitations and forge new horizons among the stars.\n",
            " Mars\n",
            "ring nations collaboratively formed the Marsium Republic, transcending geopolitical boundaries in pursuit of a shared extraterrestrial destiny. The initial colonists, comprising scientists, engineers, and adventurous pioneers, laid the foundation for the nation amidst the harsh Martian landscape.\n",
            "The culture of the Marsium Republic reflects a blend of terrestrial heritage and pioneering spirit, shaped by the challenges and opportunities of life on Mars. Cultural diversity thrives within Martian settlements, with inhabitants hailing from diverse ethnic, national, and professional backgrounds.\n",
            " districts, deliberates on legislative matters and formulates policies to promote the welfare and progress of the Martian populace. Executive authority rests with the President of the Marsium Republic, elected by popular vote, who serves as the head of state and government.\n",
            "Marsi\n",
            " operates under a democratic system of governance, with elected representatives responsible for policymaking, administration, and oversight. The Martian Constitution, ratified by early settlers, enshrines principles of equality, freedom, and sustainability, guiding the nation's development and governance.\n",
            "hen scientists and visionaries began contemplating the colonization of Mars as humanity's next frontier. Decades of technological advancements, space exploration missions, and terraforming experiments paved the way for the establishment of permanent settlements on Mars in the mid-22nd century.\n",
            "public's geography is characterized by ongoing terraforming efforts aimed at enhancing habitability and enabling sustainable agriculture and resource extraction. Martian terraforming projects involve the manipulation of planetary climate, atmosphere, and surface features to create Earth-like conditions conducive to human habitation.\n",
            "canoes to icy polar regions and ancient riverbeds. Key settlements include the capital city of Olympus, nestled near the towering Olympus Mons volcano, and the bustling metropolis of Arcadia Planitia, renowned for its fertile plains and terraformed habitats.\n",
            " inspired by the awe-inspiring vistas of the Martian frontier and the resilience of the human spirit in the face of adversity. Martian cuisine incorporates locally grown crops and hydroponically cultivated ingredients, offering a fusion of traditional Earth dishes and innovative gastronomic creations.\n",
            "ark a\n",
            "ium R\n",
            "lands\n",
            " land\n",
            "artia\n",
            "rtian\n",
            "rtian\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to rag_agent):\n",
            "\n",
            "Mars\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}